{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maryk\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a single Dataset File\n",
    "def read_dataset(file_path):\n",
    "    if file_path.lower().endswith('.csv'):\n",
    "        dataset = pd.read_csv(file_path)\n",
    "    elif file_path.lower().endswith('.xlsx'):\n",
    "        dataset = pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .csv or .xlsx file.\")\n",
    "    \n",
    "    dataset   = np.array(dataset)\n",
    "    data_train, data_test     = train_test_split(dataset, test_size=0.2, random_state=100)\n",
    "\n",
    "    x_train, y_train   = (data_train[:,:-1]).astype(str).tolist(), (data_train[:,-1]).astype(\"int32\").tolist()\n",
    "    x_test, y_test     = (data_test[:,:-1]).astype(str).tolist(), (data_test[:,-1]).astype(\"int32\").tolist()           \n",
    "    #x_train, x_test    = x_train.squeeze(), x_test.squeeze()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# If you have Train and Test Datasets separate\n",
    "def read_train_test_dataset(train_data, test_data):\n",
    "    if train_data.lower().endswith('.csv') and test_data.lower().endswith('.csv'):\n",
    "        train_data = pd.read_csv(train_data)\n",
    "        test_data = pd.read_csv(test_data)\n",
    "    elif train_data.lower().endswith('.xlsx') and test_data.lower().endswith('.xlsx'):\n",
    "        train_data = pd.read_excel(train_data)\n",
    "        test_data = pd.read_excel(test_data)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .csv or .xlsx file.\")\n",
    "    \n",
    "    train_data, test_data   = np.array(train_data), np.array(test_data)\n",
    "\n",
    "    x_train, y_train   = (train_data[:,:-1]).astype(str).tolist(), (train_data[:,-1]).astype(\"int32\").tolist()\n",
    "    x_test, y_test     = (test_data[:,:-1]).astype(str).tolist(), (test_data[:,-1]).astype(\"int32\").tolist()           \n",
    "    #x_train, x_test    = x_train.squeeze(), x_test.squeeze()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(train_texts, test_texts):\n",
    "    #Using DistilBert Pre-trained Model\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    train_encodings = tokenizer([text[0] for text in train_texts], truncation=True, padding=True)\n",
    "    test_encodings  = tokenizer([text[0] for text in test_texts], truncation=True, padding=True)\n",
    "\n",
    "    return train_encodings, test_encodings\n",
    "\n",
    "def test_tokenizer(testdata):\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "    test_encodings = tokenizer([text[0] for text in testdata], truncation=True, padding=True)\n",
    "    return test_encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumourDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.LongTensor([self.labels[idx]])  # Convert to LongTensor\n",
    "\n",
    "        return item  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parameters(output_dir, logging_dir, num_labels, train_dataset, eval_dataset):\n",
    "    #Check if GPU is available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on: {device}\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,                # output directory\n",
    "        num_train_epochs=5,              \n",
    "        per_device_train_batch_size=8,        # batch size per device during training\n",
    "        per_device_eval_batch_size=64,        # batch size for evaluation\n",
    "        warmup_steps=500,                     # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                    # strength of weight decay\n",
    "        logging_dir=logging_dir,              # directory for storing logs\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=eval_dataset            # evaluation dataset\n",
    "    )\n",
    "\n",
    "    return model, trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trained_model, custom_x_test):\n",
    "    \n",
    "    # Make predictions on the validation dataset\n",
    "    predictions = trained_model.predict(custom_x_test)\n",
    "    predictions = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "    # Extract labels from the validation dataset\n",
    "    labels = custom_x_test.labels\n",
    "    report = classification_report(labels, predictions)\n",
    "\n",
    "    return predictions, report, labels \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model_name, logging_dir, num_labels, x_data, y_data, n_splits=5):\n",
    "    # Initialization and KFold Setup\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    fold = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_reports = []\n",
    "\n",
    "    # Splitting the Data\n",
    "    for train_index, test_index in kf.split(x_data):\n",
    "        fold += 1\n",
    "        print(f\"Training fold {fold}/{n_splits}\")\n",
    "\n",
    "        x_train, x_test = x_data[train_index], x_data[test_index]\n",
    "        y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "        # Tokenizing the Data\n",
    "        train_encodings, test_encodings = tokenizer(x_train, x_test)\n",
    "\n",
    "        # Creating Pytorch Datasets\n",
    "        train_dataset = HumourDataset(train_encodings, y_train)\n",
    "        test_dataset = HumourDataset(test_encodings, y_test)\n",
    "\n",
    "        # Training the Model\n",
    "        output_dir = f\"{model_name}_fold_{fold}\"\n",
    "        model, trainer = train_parameters(output_dir, logging_dir, num_labels, train_dataset, test_dataset)\n",
    "        trainer.train()\n",
    "\n",
    "        #Evaluating the Model\n",
    "        predictions, report, labels = evaluate_model(trainer, test_dataset)\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(y_test)\n",
    "        all_reports.append(report)\n",
    "    \n",
    "    return all_predictions, all_labels, all_reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model results Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(true_label, predicted):\n",
    "    report_dict = classification_report(true_label,predicted,output_dict=True)\n",
    "\n",
    "    # Save Result Report\n",
    "    save_report = pd.DataFrame(report_dict).transpose()  # Convert the report dictionary to a DataFrame\n",
    "    save_report = save_report.round(3)                   # Round the values to a specific number of decimal places\n",
    "    save_report = save_report.astype({'support': int})   # Convert the 'support' column to integers\n",
    "    save_report.loc['accuracy', ['precision', 'recall', 'support']] = [None, None, None] # Set the accuracy row to None\n",
    "\n",
    "    return save_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model):\n",
    "    return torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affiliative and Aggressive Dataset Seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aff_agg_data_seperation(x_train, x_test, y_train, y_test):\n",
    "    # get Train and test set for aff agg seperation\n",
    "    x_train    = np.array(x_train);  y_train = np.array(y_train)\n",
    "    d2_y_data  = np.expand_dims(y_train,axis=1) #add dimension  # np.squeeze() reduce dimension\n",
    "    train_1463 = np.concatenate((x_train,d2_y_data), axis=1)\n",
    "\n",
    "    x_test     = np.array(x_test);  y_test = np.array(y_test)\n",
    "    d2_y_test  = np.expand_dims(y_test,axis=1) #add dimension  # np.squeeze() reduce dimension\n",
    "    test_1463 = np.concatenate((x_test,d2_y_test), axis=1)\n",
    "\n",
    "    train_1463 = pd.DataFrame(train_1463, columns=([\"jokes\", \"labels\"]))\n",
    "    test_1463  = pd.DataFrame(test_1463, columns=([\"jokes\", \"labels\"]))\n",
    "\n",
    "    return train_1463, test_1463\n",
    "\n",
    "def select_wanted_labels(train_data, test_data):\n",
    "    train = []\n",
    "    test  = []\n",
    "    train_data = pd.read_csv(train_data)\n",
    "    test_data  = pd.read_csv(test_data)\n",
    "    train_data = np.array(train_data);  test_data = np.array(test_data)\n",
    "\n",
    "    for example in train_data:\n",
    "        if int(example[-1]) ==2 or int(example[-1]) ==3:\n",
    "            train.append(example)    \n",
    "\n",
    "    for example in test_data:\n",
    "        if int(example[-1]) ==2 or int(example[-1]) ==3:\n",
    "            test.append(example)    \n",
    "    return train,test\n",
    "\n",
    "# Example Usage\n",
    "train_path = \"datasets/train_1463.csv\"\n",
    "test_path  = \"datasets/test_1463.csv\"\n",
    "\n",
    "training, testing  = select_wanted_labels(train_path, test_path)\n",
    "train_1463 = pd.DataFrame(training, columns=([\"jokes\", \"labels\"]))\n",
    "test_1463  = pd.DataFrame(testing, columns=([\"jokes\", \"labels\"]))\n",
    "\n",
    "# Replace all occurrences of 2 with 0 in 'labels'\n",
    "train_1463['labels'] = train_1463['labels'].replace(2, 0); train_1463['labels'] = train_1463['labels'].replace(3, 1)\n",
    "test_1463['labels']  = test_1463['labels'].replace(2, 0);  test_1463['labels']  = test_1463['labels'].replace(3, 1)\n",
    "train_1463.to_csv(\"datasets/af_ag_train_1463.csv\", index=False)\n",
    "test_1463.to_csv(\"datasets/af_ag_test_1463.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross-Validation\n",
    "humor_5class_path = \"datasets/Humour_style.xlsx\"\n",
    "x_train_5, x_test_5, y_train_5, y_test_5 = read_dataset(humor_5class_path)\n",
    "\n",
    "x_data = np.array(x_train_5)\n",
    "y_data = np.array(y_train_5)\n",
    "\n",
    "num_labels = 5\n",
    "len(x_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#cross_validation_conMatrix = confusion_matrix(labels, predictions)\n",
    "#cross_validation_conMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3272dc88294a47949bee2ccbc6f0a0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6293, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 1.624, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.14}\n",
      "{'loss': 1.6243, 'learning_rate': 3e-06, 'epoch': 0.2}\n",
      "{'loss': 1.6074, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.27}\n",
      "{'loss': 1.595, 'learning_rate': 5e-06, 'epoch': 0.34}\n",
      "{'loss': 1.5778, 'learning_rate': 6e-06, 'epoch': 0.41}\n",
      "{'loss': 1.5527, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.48}\n",
      "{'loss': 1.5259, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.54}\n",
      "{'loss': 1.5047, 'learning_rate': 9e-06, 'epoch': 0.61}\n",
      "{'loss': 1.4584, 'learning_rate': 1e-05, 'epoch': 0.68}\n",
      "{'loss': 1.3555, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.75}\n",
      "{'loss': 1.3375, 'learning_rate': 1.2e-05, 'epoch': 0.82}\n",
      "{'loss': 1.1792, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.88}\n",
      "{'loss': 1.1809, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.95}\n",
      "{'loss': 1.0963, 'learning_rate': 1.5e-05, 'epoch': 1.02}\n",
      "{'loss': 0.9722, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.09}\n",
      "{'loss': 0.9718, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.16}\n",
      "{'loss': 0.8151, 'learning_rate': 1.8e-05, 'epoch': 1.22}\n",
      "{'loss': 0.8867, 'learning_rate': 1.9e-05, 'epoch': 1.29}\n",
      "{'loss': 0.7381, 'learning_rate': 2e-05, 'epoch': 1.36}\n",
      "{'loss': 0.7856, 'learning_rate': 2.1e-05, 'epoch': 1.43}\n",
      "{'loss': 0.9767, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.5}\n",
      "{'loss': 0.9338, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.56}\n",
      "{'loss': 0.873, 'learning_rate': 2.4e-05, 'epoch': 1.63}\n",
      "{'loss': 1.0312, 'learning_rate': 2.5e-05, 'epoch': 1.7}\n",
      "{'loss': 0.8842, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.77}\n",
      "{'loss': 0.7828, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.84}\n",
      "{'loss': 0.7898, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.9}\n",
      "{'loss': 0.646, 'learning_rate': 2.9e-05, 'epoch': 1.97}\n",
      "{'loss': 0.6864, 'learning_rate': 3e-05, 'epoch': 2.04}\n",
      "{'loss': 0.7268, 'learning_rate': 3.1e-05, 'epoch': 2.11}\n",
      "{'loss': 0.6432, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.18}\n",
      "{'loss': 0.5905, 'learning_rate': 3.3e-05, 'epoch': 2.24}\n",
      "{'loss': 0.6535, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.31}\n",
      "{'loss': 0.8307, 'learning_rate': 3.5e-05, 'epoch': 2.38}\n",
      "{'loss': 0.8618, 'learning_rate': 3.6e-05, 'epoch': 2.45}\n",
      "{'loss': 0.6617, 'learning_rate': 3.7e-05, 'epoch': 2.52}\n",
      "{'loss': 0.5072, 'learning_rate': 3.8e-05, 'epoch': 2.59}\n",
      "{'loss': 0.5332, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.65}\n",
      "{'loss': 0.6209, 'learning_rate': 4e-05, 'epoch': 2.72}\n",
      "{'loss': 0.5371, 'learning_rate': 4.1e-05, 'epoch': 2.79}\n",
      "{'loss': 0.8032, 'learning_rate': 4.2e-05, 'epoch': 2.86}\n",
      "{'loss': 0.8079, 'learning_rate': 4.3e-05, 'epoch': 2.93}\n",
      "{'loss': 0.4466, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.99}\n",
      "{'loss': 0.3509, 'learning_rate': 4.5e-05, 'epoch': 3.06}\n",
      "{'loss': 0.5086, 'learning_rate': 4.600000000000001e-05, 'epoch': 3.13}\n",
      "{'loss': 0.2841, 'learning_rate': 4.7e-05, 'epoch': 3.2}\n",
      "{'loss': 0.4084, 'learning_rate': 4.8e-05, 'epoch': 3.27}\n",
      "{'loss': 0.2764, 'learning_rate': 4.9e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory DistilBERT_Models/distilBERT_5final\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.292, 'learning_rate': 5e-05, 'epoch': 3.4}\n",
      "{'loss': 0.3192, 'learning_rate': 4.787234042553192e-05, 'epoch': 3.47}\n",
      "{'loss': 0.5123, 'learning_rate': 4.574468085106383e-05, 'epoch': 3.54}\n",
      "{'loss': 0.4286, 'learning_rate': 4.3617021276595746e-05, 'epoch': 3.61}\n",
      "{'loss': 0.4066, 'learning_rate': 4.148936170212766e-05, 'epoch': 3.67}\n",
      "{'loss': 0.397, 'learning_rate': 3.936170212765958e-05, 'epoch': 3.74}\n",
      "{'loss': 0.3332, 'learning_rate': 3.723404255319149e-05, 'epoch': 3.81}\n",
      "{'loss': 0.2339, 'learning_rate': 3.5106382978723407e-05, 'epoch': 3.88}\n",
      "{'loss': 0.2273, 'learning_rate': 3.2978723404255317e-05, 'epoch': 3.95}\n",
      "{'loss': 0.226, 'learning_rate': 3.085106382978723e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0606, 'learning_rate': 2.8723404255319154e-05, 'epoch': 4.08}\n",
      "{'loss': 0.2074, 'learning_rate': 2.6595744680851064e-05, 'epoch': 4.15}\n",
      "{'loss': 0.1014, 'learning_rate': 2.446808510638298e-05, 'epoch': 4.22}\n",
      "{'loss': 0.1397, 'learning_rate': 2.2340425531914894e-05, 'epoch': 4.29}\n",
      "{'loss': 0.186, 'learning_rate': 2.0212765957446807e-05, 'epoch': 4.35}\n",
      "{'loss': 0.0804, 'learning_rate': 1.8085106382978724e-05, 'epoch': 4.42}\n",
      "{'loss': 0.1612, 'learning_rate': 1.595744680851064e-05, 'epoch': 4.49}\n",
      "{'loss': 0.0911, 'learning_rate': 1.3829787234042554e-05, 'epoch': 4.56}\n",
      "{'loss': 0.0257, 'learning_rate': 1.170212765957447e-05, 'epoch': 4.63}\n",
      "{'loss': 0.1807, 'learning_rate': 9.574468085106383e-06, 'epoch': 4.69}\n",
      "{'loss': 0.1468, 'learning_rate': 7.446808510638298e-06, 'epoch': 4.76}\n",
      "{'loss': 0.0395, 'learning_rate': 5.319148936170213e-06, 'epoch': 4.83}\n",
      "{'loss': 0.1906, 'learning_rate': 3.1914893617021277e-06, 'epoch': 4.9}\n",
      "{'loss': 0.033, 'learning_rate': 1.0638297872340427e-06, 'epoch': 4.97}\n",
      "{'train_runtime': 155.8926, 'train_samples_per_second': 37.526, 'train_steps_per_second': 4.715, 'train_loss': 0.6898482188176946, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=735, training_loss=0.6898482188176946, metrics={'train_runtime': 155.8926, 'train_samples_per_second': 37.526, 'train_steps_per_second': 4.715, 'train_loss': 0.6898482188176946, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train final model on the entire dataset and evaluate on the test dataset\n",
    "train_encodings5, test_encodings5 = tokenizer(x_train_5, x_test_5)\n",
    "train_dataset5 = HumourDataset(train_encodings5, y_train_5)\n",
    "test_dataset5  = HumourDataset(test_encodings5, y_test_5)  \n",
    "output_dir  = 'DistilBERT_Models/distilBERT_5final'\n",
    "logging_dir = 'DistilBERT_Models/distilBERT_logs_5final'\n",
    "model, trainer = train_parameters(output_dir, logging_dir, num_labels, train_dataset5, test_dataset5)\n",
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9887075a0fda4b10a96195eafad7569f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83        60\n",
      "           1       0.74      0.78      0.76        45\n",
      "           2       0.68      0.65      0.66        62\n",
      "           3       0.76      0.71      0.73        58\n",
      "           4       0.93      0.82      0.87        68\n",
      "\n",
      "    accuracy                           0.77       293\n",
      "   macro avg       0.77      0.77      0.77       293\n",
      "weighted avg       0.78      0.77      0.77       293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the final model on the Test dataset\n",
    "predictions5, result5, labels5 = evaluate_model(trainer, test_dataset5) \n",
    "print(result5)\n",
    "\n",
    "# Save Model \n",
    "save_trained_model(model)\n",
    "model.save_pretrained('DistilBERT_Models/SavedModel_5classes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DistilBERT_Models/Tokenizer_5classes/tokenizer_config.json',\n",
       " 'DistilBERT_Models/Tokenizer_5classes/special_tokens_map.json',\n",
       " 'DistilBERT_Models/Tokenizer_5classes/vocab.txt',\n",
       " 'DistilBERT_Models/Tokenizer_5classes/added_tokens.json',\n",
       " 'DistilBERT_Models/Tokenizer_5classes/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "new_tokenizer.save_pretrained('DistilBERT_Models/Tokenizer_5classes/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55,  4,  1,  0,  0],\n",
       "       [ 5, 35,  0,  4,  1],\n",
       "       [10,  5, 40,  6,  1],\n",
       "       [ 3,  2, 10, 41,  2],\n",
       "       [ 0,  1,  8,  3, 56]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conMatrix = confusion_matrix(labels5, predictions5)\n",
    "test_conMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Report and write to CSV\n",
    "distilBERT_result_5= save_results(y_test_5,predictions5)\n",
    "distilBERT_result_5.to_csv('models_results/distilBERT_5classes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage for Cross-Validation\n",
    "humor_4class_path = \"datasets/Humour_style_4classes.xlsx\" \n",
    "x_train_4, x_test_4, y_train_4, y_test_4 = read_dataset(humor_4class_path)\n",
    "\n",
    "x_data4 = np.array(x_train_4)\n",
    "y_data4 = np.array(y_train_4)\n",
    "\n",
    "num_labels4= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions4, labels4, reports4 = cross_validate('DistilBERT_Models/distilBERT_4classes', 'DistilBERT_Models/distilBERT_logs_4classes', num_labels4, x_data4, y_data4)\n",
    "\n",
    "# Save the cross-validation results\n",
    "#final_report4 = save_results(labels4, predictions4)\n",
    "#final_report4.to_csv('cross_validation_results/distilBERT_4classes.csv', index=True)\n",
    "#print(final_report4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab5fde6a13c4c6f94e90a85e9f56a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4016, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.07}\n",
      "{'loss': 1.377, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.14}\n",
      "{'loss': 1.4026, 'learning_rate': 3e-06, 'epoch': 0.2}\n",
      "{'loss': 1.391, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.27}\n",
      "{'loss': 1.345, 'learning_rate': 5e-06, 'epoch': 0.34}\n",
      "{'loss': 1.3434, 'learning_rate': 6e-06, 'epoch': 0.41}\n",
      "{'loss': 1.3274, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.48}\n",
      "{'loss': 1.3083, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.54}\n",
      "{'loss': 1.2608, 'learning_rate': 9e-06, 'epoch': 0.61}\n",
      "{'loss': 1.1827, 'learning_rate': 1e-05, 'epoch': 0.68}\n",
      "{'loss': 1.0956, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.75}\n",
      "{'loss': 1.0676, 'learning_rate': 1.2e-05, 'epoch': 0.82}\n",
      "{'loss': 0.9518, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.88}\n",
      "{'loss': 0.8785, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.95}\n",
      "{'loss': 0.8851, 'learning_rate': 1.5e-05, 'epoch': 1.02}\n",
      "{'loss': 0.7282, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.09}\n",
      "{'loss': 0.7948, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.16}\n",
      "{'loss': 0.5855, 'learning_rate': 1.8e-05, 'epoch': 1.22}\n",
      "{'loss': 0.5987, 'learning_rate': 1.9e-05, 'epoch': 1.29}\n",
      "{'loss': 0.5537, 'learning_rate': 2e-05, 'epoch': 1.36}\n",
      "{'loss': 0.6276, 'learning_rate': 2.1e-05, 'epoch': 1.43}\n",
      "{'loss': 0.7566, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.5}\n",
      "{'loss': 0.7257, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.56}\n",
      "{'loss': 0.6608, 'learning_rate': 2.4e-05, 'epoch': 1.63}\n",
      "{'loss': 0.8048, 'learning_rate': 2.5e-05, 'epoch': 1.7}\n",
      "{'loss': 0.5475, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.77}\n",
      "{'loss': 0.5586, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.84}\n",
      "{'loss': 0.5749, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.9}\n",
      "{'loss': 0.418, 'learning_rate': 2.9e-05, 'epoch': 1.97}\n",
      "{'loss': 0.434, 'learning_rate': 3e-05, 'epoch': 2.04}\n",
      "{'loss': 0.564, 'learning_rate': 3.1e-05, 'epoch': 2.11}\n",
      "{'loss': 0.4978, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.18}\n",
      "{'loss': 0.5674, 'learning_rate': 3.3e-05, 'epoch': 2.24}\n",
      "{'loss': 0.4942, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.31}\n",
      "{'loss': 0.6477, 'learning_rate': 3.5e-05, 'epoch': 2.38}\n",
      "{'loss': 0.5265, 'learning_rate': 3.6e-05, 'epoch': 2.45}\n",
      "{'loss': 0.3507, 'learning_rate': 3.7e-05, 'epoch': 2.52}\n",
      "{'loss': 0.2768, 'learning_rate': 3.8e-05, 'epoch': 2.59}\n",
      "{'loss': 0.4402, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.65}\n",
      "{'loss': 0.3066, 'learning_rate': 4e-05, 'epoch': 2.72}\n",
      "{'loss': 0.3863, 'learning_rate': 4.1e-05, 'epoch': 2.79}\n",
      "{'loss': 0.682, 'learning_rate': 4.2e-05, 'epoch': 2.86}\n",
      "{'loss': 0.5422, 'learning_rate': 4.3e-05, 'epoch': 2.93}\n",
      "{'loss': 0.4619, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.99}\n",
      "{'loss': 0.2508, 'learning_rate': 4.5e-05, 'epoch': 3.06}\n",
      "{'loss': 0.4339, 'learning_rate': 4.600000000000001e-05, 'epoch': 3.13}\n",
      "{'loss': 0.208, 'learning_rate': 4.7e-05, 'epoch': 3.2}\n",
      "{'loss': 0.2815, 'learning_rate': 4.8e-05, 'epoch': 3.27}\n",
      "{'loss': 0.4059, 'learning_rate': 4.9e-05, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory DistilBERT_Models/distilBERT_4classes\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1334, 'learning_rate': 5e-05, 'epoch': 3.4}\n",
      "{'loss': 0.2118, 'learning_rate': 4.787234042553192e-05, 'epoch': 3.47}\n",
      "{'loss': 0.4177, 'learning_rate': 4.574468085106383e-05, 'epoch': 3.54}\n",
      "{'loss': 0.4331, 'learning_rate': 4.3617021276595746e-05, 'epoch': 3.61}\n",
      "{'loss': 0.4618, 'learning_rate': 4.148936170212766e-05, 'epoch': 3.67}\n",
      "{'loss': 0.2501, 'learning_rate': 3.936170212765958e-05, 'epoch': 3.74}\n",
      "{'loss': 0.3192, 'learning_rate': 3.723404255319149e-05, 'epoch': 3.81}\n",
      "{'loss': 0.1795, 'learning_rate': 3.5106382978723407e-05, 'epoch': 3.88}\n",
      "{'loss': 0.2005, 'learning_rate': 3.2978723404255317e-05, 'epoch': 3.95}\n",
      "{'loss': 0.2668, 'learning_rate': 3.085106382978723e-05, 'epoch': 4.01}\n",
      "{'loss': 0.0646, 'learning_rate': 2.8723404255319154e-05, 'epoch': 4.08}\n",
      "{'loss': 0.1874, 'learning_rate': 2.6595744680851064e-05, 'epoch': 4.15}\n",
      "{'loss': 0.1186, 'learning_rate': 2.446808510638298e-05, 'epoch': 4.22}\n",
      "{'loss': 0.1348, 'learning_rate': 2.2340425531914894e-05, 'epoch': 4.29}\n",
      "{'loss': 0.1212, 'learning_rate': 2.0212765957446807e-05, 'epoch': 4.35}\n",
      "{'loss': 0.0407, 'learning_rate': 1.8085106382978724e-05, 'epoch': 4.42}\n",
      "{'loss': 0.1487, 'learning_rate': 1.595744680851064e-05, 'epoch': 4.49}\n",
      "{'loss': 0.0341, 'learning_rate': 1.3829787234042554e-05, 'epoch': 4.56}\n",
      "{'loss': 0.1011, 'learning_rate': 1.170212765957447e-05, 'epoch': 4.63}\n",
      "{'loss': 0.1786, 'learning_rate': 9.574468085106383e-06, 'epoch': 4.69}\n",
      "{'loss': 0.0775, 'learning_rate': 7.446808510638298e-06, 'epoch': 4.76}\n",
      "{'loss': 0.1335, 'learning_rate': 5.319148936170213e-06, 'epoch': 4.83}\n",
      "{'loss': 0.0635, 'learning_rate': 3.1914893617021277e-06, 'epoch': 4.9}\n",
      "{'loss': 0.0647, 'learning_rate': 1.0638297872340427e-06, 'epoch': 4.97}\n",
      "{'train_runtime': 154.6499, 'train_samples_per_second': 37.827, 'train_steps_per_second': 4.753, 'train_loss': 0.547748986753274, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=735, training_loss=0.547748986753274, metrics={'train_runtime': 154.6499, 'train_samples_per_second': 37.827, 'train_steps_per_second': 4.753, 'train_loss': 0.547748986753274, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Usage\n",
    "train_encodings4, test_encodings4 = tokenizer(x_train_4, x_test_4)\n",
    "train_dataset4 = HumourDataset(train_encodings4, y_train_4)\n",
    "test_dataset4  = HumourDataset(test_encodings4, y_test_4)  \n",
    "output_dir4  = 'DistilBERT_Models/distilBERT_4classes'\n",
    "logging_dir4 = 'DistilBERT_Models/distilBERT_logs_4classes'\n",
    "num_labels4= 4\n",
    "model4, trainer4 = train_parameters(output_dir4, logging_dir4, num_labels4, train_dataset4, test_dataset4)\n",
    "trainer4.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0645ae5cee3f432cabec3f5f6890dc31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80        60\n",
      "           1       0.84      0.69      0.76        45\n",
      "           2       0.84      0.82      0.83       120\n",
      "           3       0.89      0.87      0.88        68\n",
      "\n",
      "    accuracy                           0.83       293\n",
      "   macro avg       0.83      0.82      0.82       293\n",
      "weighted avg       0.83      0.83      0.83       293\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[53,  3,  4,  0],\n",
       "       [ 6, 31,  7,  1],\n",
       "       [13,  2, 99,  6],\n",
       "       [ 0,  1,  8, 59]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and Evaluate Model \n",
    "predictions4, result4, labels4 = evaluate_model(trainer4, test_dataset4) \n",
    "\n",
    "model4.save_pretrained('DistilBERT_Models/SavedModel_4finalmodel/')\n",
    "print(result4)\n",
    "\n",
    "# Save Report\n",
    "distilBERT_result_4= save_results(y_test_4,predictions4)\n",
    "\n",
    "# Confusion matrix\n",
    "test_conMatrix4 = confusion_matrix(labels4, predictions4)\n",
    "test_conMatrix4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Report to CSV\n",
    "distilBERT_result_4.to_csv('models_results/distilBERT_4classes.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage for Cross-Validation\n",
    "# train_2class_path = \"datasets/af_ag_train.xlsx\" ; test_2class_path  = \"datasets/af_ag_test.xlsx\" \n",
    "train_2class_path = \"datasets/af_ag_train_1463.csv\" ; test_2class_path  = \"datasets/af_ag_test_1463.csv\" \n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = read_train_test_dataset(train_2class_path, test_2class_path)\n",
    "\n",
    "x_data2 = np.array(x_train_2)\n",
    "y_data2 = np.array(y_train_2)\n",
    "\n",
    "num_labels2= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions2, labels2, reports2 = cross_validate('DistilBERT_Models/distilBERT_2classes', 'DistilBERT_Models/distilBERT_logs_2classes', num_labels2, x_data2, y_data2)\n",
    "\n",
    "# Save the cross-validation results\n",
    "#final_report2 = save_results(labels2, predictions2)\n",
    "#final_report2.to_csv('cross_validation_results/distilBERT_2classes.csv', index=True)\n",
    "#print(final_report2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Training of two class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdac4051c684e0286d7e394e7491e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/280 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6918, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.18}\n",
      "{'loss': 0.6861, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.36}\n",
      "{'loss': 0.69, 'learning_rate': 3e-06, 'epoch': 0.54}\n",
      "{'loss': 0.6879, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.71}\n",
      "{'loss': 0.6937, 'learning_rate': 5e-06, 'epoch': 0.89}\n",
      "{'loss': 0.6654, 'learning_rate': 6e-06, 'epoch': 1.07}\n",
      "{'loss': 0.6704, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.25}\n",
      "{'loss': 0.6641, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.43}\n",
      "{'loss': 0.6274, 'learning_rate': 9e-06, 'epoch': 1.61}\n",
      "{'loss': 0.6478, 'learning_rate': 1e-05, 'epoch': 1.79}\n",
      "{'loss': 0.6395, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.96}\n",
      "{'loss': 0.5688, 'learning_rate': 1.2e-05, 'epoch': 2.14}\n",
      "{'loss': 0.539, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.32}\n",
      "{'loss': 0.4938, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.5}\n",
      "{'loss': 0.4454, 'learning_rate': 1.5e-05, 'epoch': 2.68}\n",
      "{'loss': 0.3763, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.86}\n",
      "{'loss': 0.4618, 'learning_rate': 1.7000000000000003e-05, 'epoch': 3.04}\n",
      "{'loss': 0.2604, 'learning_rate': 1.8e-05, 'epoch': 3.21}\n",
      "{'loss': 0.3197, 'learning_rate': 1.9e-05, 'epoch': 3.39}\n",
      "{'loss': 0.3807, 'learning_rate': 2e-05, 'epoch': 3.57}\n",
      "{'loss': 0.2871, 'learning_rate': 2.1e-05, 'epoch': 3.75}\n",
      "{'loss': 0.3596, 'learning_rate': 2.2000000000000003e-05, 'epoch': 3.93}\n",
      "{'loss': 0.2302, 'learning_rate': 2.3000000000000003e-05, 'epoch': 4.11}\n",
      "{'loss': 0.1151, 'learning_rate': 2.4e-05, 'epoch': 4.29}\n",
      "{'loss': 0.128, 'learning_rate': 2.5e-05, 'epoch': 4.46}\n",
      "{'loss': 0.2314, 'learning_rate': 2.6000000000000002e-05, 'epoch': 4.64}\n",
      "{'loss': 0.1031, 'learning_rate': 2.7000000000000002e-05, 'epoch': 4.82}\n",
      "{'loss': 0.2243, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.0}\n",
      "{'train_runtime': 58.3511, 'train_samples_per_second': 38.388, 'train_steps_per_second': 4.799, 'train_loss': 0.46031890639236994, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=280, training_loss=0.46031890639236994, metrics={'train_runtime': 58.3511, 'train_samples_per_second': 38.388, 'train_steps_per_second': 4.799, 'train_loss': 0.46031890639236994, 'epoch': 5.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Usage\n",
    "train_encodings2, test_encodings2 = tokenizer(x_train_2, x_test_2)\n",
    "train_dataset2 = HumourDataset(train_encodings2, y_train_2)\n",
    "test_dataset2  = HumourDataset(test_encodings2, y_test_2)  \n",
    "output_dir2  = 'DistilBERT_Models/distilBERT_2classes'\n",
    "logging_dir2 = 'DistilBERT_Models/distilBERT_logs_2classes'\n",
    "model2, trainer2 = train_parameters(output_dir2, logging_dir2, num_labels2, train_dataset2, test_dataset2)\n",
    "trainer2.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab7b2eabf3a4b548100fd183cf52ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80        62\n",
      "           1       0.78      0.79      0.79        58\n",
      "\n",
      "    accuracy                           0.79       120\n",
      "   macro avg       0.79      0.79      0.79       120\n",
      "weighted avg       0.79      0.79      0.79       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save and Evaluate Model \n",
    "model2.save_pretrained('DistilBERT_Models/SavedModel_2classes/')\n",
    "predictions2, result2, labels2 = evaluate_model(trainer2, test_dataset2) \n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Report\n",
    "distilBERT_result_2= save_results(y_test_2,predictions2)\n",
    "distilBERT_result_2.to_csv('models_results/distilBERT_2classes.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "new_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_prediction(text):\n",
    "    encoding = new_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    encoding = {k: v.to(device) for k,v in encoding.items()}  # Ensure tensors are on the correct device\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**encoding)\n",
    "\n",
    "    # Get the logits from the output\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply Softmax to get probabilities\n",
    "    softmax = torch.nn.Softmax(dim=0)  # Assuming the logits are along dim=0 (num_labels)\n",
    "    probs = softmax(logits.squeeze())\n",
    "\n",
    "    # Convert probabilities to numpy array\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    # Get the label with highest probability\n",
    "    label = np.argmax(probs)\n",
    "\n",
    "    # Define the style based on the label\n",
    "    if label == 0:\n",
    "        return {\n",
    "            'style': 'Self-enhancing',\n",
    "            'probability': probs[0]\n",
    "        }\n",
    "    elif label == 1:\n",
    "        return {\n",
    "            'style': 'Self-deprecating',\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    elif label == 2 and probs[2] > 0.65:\n",
    "        return {\n",
    "            'style': 'Affiliative',\n",
    "            'probability': probs[2]\n",
    "        }\n",
    "    elif label == 2 and probs[2] <= 0.65:\n",
    "        return {\n",
    "            'style': 'Aggressive',\n",
    "            'probability': probs[2]\n",
    "        }\n",
    "    elif label == 3:\n",
    "        return {\n",
    "            'style': 'Aggressive',\n",
    "            'probability': probs[3]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'style': 'Neutral',\n",
    "            'probability': probs[4]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'style': 'Aggressive', 'probability': 0.7611798}\n",
      "{'style': 'Aggressive', 'probability': 0.9223928}\n",
      "{'style': 'Aggressive', 'probability': 0.7374662}\n",
      "{'style': 'Aggressive', 'probability': 0.56487465}\n",
      "{'style': 'Affiliative', 'probability': 0.97560364}\n",
      "{'style': 'Neutral', 'probability': 0.99481237}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "print(get_prediction(\"Q: Why do Jewish men get circumcised? A: Because Jewish women won't touch anything unless it's 20 percent off.\"))\n",
    "print(get_prediction(\"Q: Why are all black people fast? A: The slow ones are in jail.\"))\n",
    "print(get_prediction(\"How can you tell a black guy has been on your computer? It's not there.\"))\n",
    "print(get_prediction(\"Q: Why can't Mexicans play Uno? A: They always steal the green cards.\"))\n",
    "print(get_prediction(\"Hitler calls a meeting of his best soldiers and commanders and tells them ,Alright I want to order the assassination of one thousand jews and four hedgehogs.Then one of his generals stands and says, But... Mein furhur why four hedgehogs? Hitler then smiles and says See? No one gives a f*ck about the jews.\"))\n",
    "print(get_prediction(\"Mary is Fat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "new_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_prediction_2(text):\n",
    "    encoding = new_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    encoding = {k: v.to(device) for k,v in encoding.items()}  # Ensure tensors are on the correct device\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model2(**encoding)\n",
    "\n",
    "    # Get the logits from the output\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply Softmax to get probabilities\n",
    "    softmax = torch.nn.Softmax(dim=0)  # Assuming the logits are along dim=0 (num_labels)\n",
    "    probs = softmax(logits.squeeze())\n",
    "\n",
    "    # Convert probabilities to numpy array\n",
    "    probs = probs.detach().cpu().numpy()\n",
    "\n",
    "    # Get the label with highest probability\n",
    "    label = np.argmax(probs)\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(predictions4))\n",
    "model4_pred = predictions4\n",
    "aff_agg_dataset = []\n",
    "agg_agg_label = []\n",
    "\n",
    "# Identify instances classified as affiliative/aggressive\n",
    "aff_agg_mask = (model4_pred == 2)\n",
    "\n",
    "for i in range(len(predictions4)):\n",
    "    if predictions4[i] == 2:\n",
    "        aff_agg_dataset.append(x_test_4[i])\n",
    "        agg_agg_label.append(predictions4[i])\n",
    "\n",
    "#for i in aff_agg_dataset:\n",
    "#    i = str(i)\n",
    "#    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_model_predictions = []\n",
    "for i in aff_agg_dataset:\n",
    "    i = str(i)\n",
    "    two_model_predictions.append(get_prediction_2(i))\n",
    "\n",
    "# Combine results\n",
    "final_pred = [4 if pred == 3 else pred for pred in model4_pred]\n",
    "model2_pred = [2 if p == 0 else 3 for p in two_model_predictions]\n",
    "\n",
    "# Update final_pred with model2_pred results\n",
    "pred2_index = 0\n",
    "for i, mask in enumerate(aff_agg_mask):\n",
    "    if mask:\n",
    "        final_pred[i] = model2_pred[pred2_index]\n",
    "        pred2_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1]\n",
      "[2, 0, 1, 3, 2, 0, 4, 0, 2, 1, 1, 4, 4, 0, 1, 1, 1, 2, 0, 0, 0, 2, 4, 3, 2, 2, 0, 1, 3, 2, 4, 4, 4, 2, 0, 0, 0, 3, 4, 0, 1, 4, 1, 0, 2, 0, 4, 4, 3, 2, 2, 0, 0, 2, 2, 1, 0, 0, 4, 2, 4, 3, 3, 3, 2, 2, 4, 2, 0, 0, 0, 0, 4, 1, 0, 2, 1, 2, 1, 4, 4, 1, 1, 1, 3, 0, 1, 4, 3, 1, 1, 1, 2, 3, 0, 0, 2, 4, 0, 0, 2, 0, 2, 4, 4, 1, 3, 0, 0, 4, 2, 0, 2, 3, 2, 0, 3, 0, 4, 4, 0, 2, 4, 4, 3, 1, 2, 2, 2, 0, 3, 1, 0, 3, 4, 4, 2, 4, 3, 4, 2, 3, 4, 2, 1, 4, 0, 0, 2, 0, 4, 0, 1, 4, 4, 1, 2, 0, 1, 2, 0, 1, 4, 0, 1, 3, 4, 4, 3, 0, 3, 0, 3, 2, 2, 4, 2, 4, 0, 0, 3, 4, 2, 3, 3, 4, 2, 1, 2, 2, 0, 4, 4, 3, 2, 3, 0, 2, 0, 4, 0, 2, 0, 0, 0, 2, 4, 3, 1, 4, 3, 4, 2, 0, 2, 0, 2, 0, 4, 1, 4, 2, 0, 0, 4, 0, 0, 1, 3, 2, 4, 4, 0, 4, 4, 3, 4, 4, 2, 3, 4, 0, 4, 0, 3, 4, 2, 0, 2, 1, 2, 2, 3, 3, 0, 2, 4, 2, 2, 2, 2, 4, 4, 2, 2, 2, 1, 0, 3, 4, 0, 2, 4, 3, 2, 3, 3, 2, 0, 4, 1, 0, 2, 1, 3, 3, 0, 3, 2, 3, 3, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(two_model_predictions)\n",
    "print(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "              precision  recall  f1-score  support\n",
      "0                 0.736   0.883     0.803     60.0\n",
      "1                 0.838   0.689     0.756     45.0\n",
      "2                 0.569   0.661     0.612     62.0\n",
      "3                 0.804   0.638     0.712     58.0\n",
      "4                 0.894   0.868     0.881     68.0\n",
      "accuracy            NaN     NaN     0.754      NaN\n",
      "macro avg         0.768   0.748     0.753    293.0\n",
      "weighted avg      0.767   0.754     0.755    293.0\n"
     ]
    }
   ],
   "source": [
    "two_model_result= save_results(y_test_5,final_pred)\n",
    "\n",
    "print(type(aff_agg_dataset))\n",
    "print(type(x_test_2))\n",
    "print(type(x_test_4))\n",
    "print(two_model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_model_result.to_csv(\"two_model_pipeline_results/distilBERT.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
